{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6170331b",
   "metadata": {},
   "source": [
    "<b><h1><span style=\"color:Orange\">Synthetic Data Generation using GAN's</span></h1></b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef92eae",
   "metadata": {},
   "source": [
    "To get started with the task of Synthetic Data Generation, we need a dataset that we can use to feed into a Generative Adversarial Networks (GANs) model, which will be trained to generate new data samples that will be similar to the original data and the relationships between the features in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03616f54",
   "metadata": {},
   "source": [
    "The dataset contains daily records of insights into app usage patterns over time. The goal will be to generate synthetic data that mimics the original dataset by ensuring that it maintains the same statistical properties while providing privacy for users actual usage behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbc463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>App</th>\n",
       "      <th>Usage (minutes)</th>\n",
       "      <th>Notifications</th>\n",
       "      <th>Times Opened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-07</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>81</td>\n",
       "      <td>24</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-26</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>112</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-22</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>82</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-12</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        App  Usage (minutes)  Notifications  Times Opened\n",
       "0  2024-08-07  Instagram               81             24            57\n",
       "1  2024-08-08  Instagram               90             30            53\n",
       "2  2024-08-26  Instagram              112             33            17\n",
       "3  2024-08-22  Instagram               82             11            38\n",
       "4  2024-08-12  Instagram               59             47            16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('C:\\\\Users\\\\hp\\\\Downloads\\\\screentime_analysis.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef9ce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Date             200 non-null    object\n",
      " 1   App              200 non-null    object\n",
      " 2   Usage (minutes)  200 non-null    int64 \n",
      " 3   Notifications    200 non-null    int64 \n",
      " 4   Times Opened     200 non-null    int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info() #Column summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440fd413",
   "metadata": {},
   "source": [
    "Date: The date of the screentime data.\n",
    "Usage: Total usage time of the app (likely in minutes).\n",
    "Notifications: The number of notifications received.\n",
    "Times opened: The number of times the app was opened.\n",
    "App: The name of the app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42266adf",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1886647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage (minutes)</th>\n",
       "      <th>Notifications</th>\n",
       "      <th>Times Opened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.754237</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940678</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.163265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.686441</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.377551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.153061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Usage (minutes)  Notifications  Times Opened\n",
       "0         0.677966       0.163265      0.571429\n",
       "1         0.754237       0.204082      0.530612\n",
       "2         0.940678       0.224490      0.163265\n",
       "3         0.686441       0.074830      0.377551\n",
       "4         0.491525       0.319728      0.153061"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "data_gan = data.drop(columns=['Date', 'App'])\n",
    "\n",
    "# initialize a MinMaxScaler to normalize the data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# normalize the data\n",
    "normalized_data = scaler.fit_transform(data_gan)\n",
    "\n",
    "# convert back to a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=data_gan.columns)\n",
    "\n",
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f17882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The generator will take a latent noise vector as input and generate a synthetic sample similar to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ce4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               12928     \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,659\n",
      "Trainable params: 180,867\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100  # latent space dimension (size of the random noise input vector)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=latent_dim),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(3, activation='sigmoid')  # output layer for generating 3 features\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#create the generator\n",
    "generator = build_generator(latent_dim)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b6edb",
   "metadata": {},
   "source": [
    "Now, we will build a discriminator that will take a real or synthetic data sample and classify it as real or fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869118c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               2048      \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,401\n",
      "Trainable params: 166,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Dense(512, input_shape=(3,)),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(1, activation='sigmoid')  # output: 1 neuron for real/fake classification\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#create the discriminator\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3878e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we will freeze the discriminator’s weights when training the generator to ensure only the generator is updated during those training steps.This is crucial so that there is no critic from the discriminator during the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578a0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 3)                 182659    \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1)                 166401    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 349,060\n",
      "Trainable params: 180,867\n",
      "Non-trainable params: 168,193\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    # freeze the discriminator’s weights while training the generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential([generator, discriminator])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "    return model\n",
    "\n",
    "# create the GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae307e56",
   "metadata": {},
   "source": [
    "Now, we will train the GAN using the following steps:\n",
    "\n",
    "1.Generate random noise.\n",
    "2.Use the generator to create fake data.\n",
    "3.Train the discriminator on both real and fake data.\n",
    "4.Train the generator via the GAN to fool the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a81397fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D Loss: [0.72309381 0.        ], G Loss: 0.6651806831359863\n",
      "Epoch 1000: D Loss: [0.68430826 0.5703125 ], G Loss: 0.6891820430755615\n",
      "Epoch 2000: D Loss: [0.67301723 0.625     ], G Loss: 0.7564046382904053\n",
      "Epoch 3000: D Loss: [0.62184837 0.6953125 ], G Loss: 0.7539350390434265\n",
      "Epoch 4000: D Loss: [0.62850484 0.765625  ], G Loss: 0.8381952047348022\n",
      "Epoch 5000: D Loss: [0.69159091 0.4765625 ], G Loss: 0.6588637828826904\n",
      "Epoch 6000: D Loss: [0.69670784 0.5234375 ], G Loss: 0.7726343870162964\n",
      "Epoch 7000: D Loss: [0.72762233 0.47265625], G Loss: 0.7929559946060181\n",
      "Epoch 8000: D Loss: [0.65594995 0.6015625 ], G Loss: 0.762755811214447\n",
      "Epoch 9000: D Loss: [0.65930718 0.59375   ], G Loss: 0.7420365810394287\n"
     ]
    }
   ],
   "source": [
    "def train_gan(gan, generator, discriminator, data, epochs=10000, batch_size=128, latent_dim=100):\n",
    "    for epoch in range(epochs):\n",
    "        # select a random batch of real data\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "        real_data = data[idx]\n",
    "\n",
    "        # generate a batch of fake data\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        fake_data = generator.predict(noise)\n",
    "\n",
    "        # labels for real and fake data\n",
    "        real_labels = np.ones((batch_size, 1))  # real data has label 1\n",
    "        fake_labels = np.zeros((batch_size, 1))  # fake data has label 0\n",
    "\n",
    "        # train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "\n",
    "        # train the generator via the GAN\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1)) #labelled as 1 so discriminator classifies it as real\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        # print the progress every 1000 epochs\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}: D Loss: {0.5 * np.add(d_loss_real, d_loss_fake)}, G Loss: {g_loss}\")\n",
    "\n",
    "train_gan(gan, generator, discriminator, normalized_data, epochs=10000, batch_size=128, latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a95c5",
   "metadata": {},
   "source": [
    "#### Now, here’s how we can use the generator to create new synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a1f8b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage (minutes)</th>\n",
       "      <th>Notifications</th>\n",
       "      <th>Times Opened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.178711</td>\n",
       "      <td>0.454729</td>\n",
       "      <td>4.242694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.332947</td>\n",
       "      <td>50.011417</td>\n",
       "      <td>42.835747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.794147</td>\n",
       "      <td>146.813004</td>\n",
       "      <td>71.754539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.411764</td>\n",
       "      <td>10.627835</td>\n",
       "      <td>5.069164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.073372</td>\n",
       "      <td>44.249798</td>\n",
       "      <td>69.020828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Usage (minutes)  Notifications  Times Opened\n",
       "0        68.178711       0.454729      4.242694\n",
       "1        47.332947      50.011417     42.835747\n",
       "2        63.794147     146.813004     71.754539\n",
       "3        21.411764      10.627835      5.069164\n",
       "4        69.073372      44.249798     69.020828"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new data\n",
    "noise = np.random.normal(0, 1, (1000, latent_dim))  # generate 1000 synthetic samples\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "# convert the generated data back to the original scale\n",
    "generated_data_rescaled = scaler.inverse_transform(generated_data)\n",
    "\n",
    "# convert to DataFrame\n",
    "generated_df = pd.DataFrame(generated_data_rescaled, columns=data_gan.columns)\n",
    "\n",
    "generated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74e0be",
   "metadata": {},
   "source": [
    "<b><h3><span style=\"color:Green\">In this project, we explored the task of synthetic data generation with Generative AI using Generative Adversarial Networks (GANs). We started by preprocessing a dataset of app usage insights by focusing on features like Usage, Notifications, and Times opened which were normalized for GAN training. The GAN architecture was built with a generator to create synthetic data and a discriminator to distinguish between real and generated data</span></h3></b> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
